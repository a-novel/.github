# Testing

This document explains how to write unit tests in Go for A-Novel services using the table-driven testing pattern. It also covers mocking dependencies and best practices to ensure tests are maintainable, reliable, and clear.

## Overview

In our codebase we rely on Go's built-in `testing` package, combined with table-driven tests and mocks for isolating dependencies.

**Table-driven tests** let you define multiple cases in a structured table, avoiding boilerplate and making tests easier to read and extend.

**Mocks** replace external or lower-layer dependencies (like DAOs, external services or connectors) so your test only exercises the business logic you care about. We use [mockery](https://github.com/vektra/mockery) for generating mocks of interfaces.

## Test File Conventions

- Tests live in the same package as the code they test.
- Files with tests must end in `_test.go`.

## Unit tests

One unit test covers one method in its layer. You may occasionally encounter multiple test functions for a single
method. This is allowed when testing poorly predictable methods but should remain an exception. Mostly, such tests
are encountered under the `/lib` layer, where rules that apply to standard layers are less enforced.

```go
package layer_test

import "testing"

func TestMyMethod(t *testing.T) {
	t.Parallel()

	// ...
}
```

Tests (and subtests) must run in parallel whenever possible. The only exceptions are DAO and integration tests,
which have race-lock issues.

> It is CRUCIAL that the package in which tests lives ends with the suffix `_test`. This is an allowed exception to
> go's package naming convention, which usually only allows one package per directory. This puts tests in a separate
> package, so internal fields or unexported implementations are not visible to the tests. This is important to make
> sure the test accurately runs the code from an outside perspective.

### Writing table-driven tests

Below is the generic anatomy of a table-driven test.

```go
package layer_test

import "testing"

func TestMyMethod(t *testing.T) {
	t.Parallel()

	errFoo := errors.New("foo")

	type dependencyMock1 struct {
		req  *Dependency1Request
		resp *Dependency1Response
		err  error
	}

	type dependencyMock2 struct {
		req  *Dependency2Request
		resp *Dependency2Response
		err  error
	}

	testCases := []struct {
		name string

		input *MyInput

		dependencyMock1 *dependencyMock1
		dependencyMock2 *dependencyMock2

		expect *MyOutput
		expectErr error
	}{
		// ... Test cases.
	}

	for _, testCase := range testCases {
		t.Run(testCase.name, func(t *testing.T) {
			t.Parallel()

			ctx := t.Context()

			dependency1 := &layermocks.NewMockDependency1(t)
			dependency2 := &layermocks.NewMockDependency2(t)

			if testCase.dependencyMock1 != nil {
				dependency1.EXPECT().
					Exec(mock.Anything, testCase.dependencyMock1.req).
					Return(testCase.dependencyMock1.resp, testCase.dependencyMock1.err)
			}

			if testCase.dependencyMock2 != nil {
				dependency2.EXPECT().
					Exec(mock.Anything, testCase.dependencyMock2.req).
					Return(testCase.dependencyMock2.resp, testCase.dependencyMock2.err)
			}

			instance := layer.NewCredentialsCreate(dependency1, dependency2)

			resp, err := instance.Exec(ctx, testCase.request)
			require.ErrorIs(t, err, testCase.expectErr)
			require.Equal(t, testCase.expect, resp)

			dependency1.assertExpectations(t)
			dependency2.assertExpectations(t)
		})
	}
}
```

While some implementation details may vary across layers, you always need the same base components:

- A generic `errFoo`, used to test the response to unexpected errors (see below)
- Dependency mocks: each test case should define what the mock expects as an input, and what it produces as an output.
  The mock definition is a struct named `[dependencyName]Mock` (e.g. "moduleCreateRepositoryMock"), which contains
  fields for all the expected inputs and outputs for the given test case. If a given dependency is not excpected to
  be called, the test can pass a nil value.
- The test cases: a list that always use the same structure.
  - `name` of the test
  - expected inputs
  - expected dependency calls
  - expected outputs (usually `expect` for the base output, and `expectErr` for the error)
- The test cases loop
  - Runs a subtest
  - Create the context (and populate it if necessary)
  - Create the mocked dependencies instances (and sets them up, see
    [mockery's doc](https://vektra.github.io/mockery/latest/template/testify/))
  - Create the instance (service, handler, repository), call its exec method
  - Assert error and output
  - Assert mocks were properly called

#### Test cases

Writing a test case is straightforward: set your input, your mocks, and assert the output. What is more complicated
is what tests to write. Here is the methodology to follow to write optimal tests:

##### 1. Start with success cases

You first and foremost want to ensure your code works under normal conditions. You may ensure edge cases work as well
(for example, optional values are empty, or other conditional). Those are your "success" cases, and should be placed
on top.

```go
testCases := []struct {
	name string

	// ...
}{
	{
		name: "Success",

		// ...
	},
	{
		name: "Success/ValueEmpty",

		// ...
	}
}
```

> While go tests allow any name format, they are converted in the console (spaces are replaced by underscores, and
> the test function is prepended to the test name -> TestMyFunction/sub_test_name). To remain consistent, we recommend
> using the same naming convention for both test functions and subtests, i.e. CamelCase, and slashes for namespacing.

##### 2. Write error cases

Now, you want to try how your code behaves under unexpected conditions. Errors can come from 2 sources: dependencies,
or libraries (e.g. struct validation). Tests also always occur in the same part of your workflow.

Let's say your code does the following:

- Validate the input
- Call dependency 1
- Call dependency 2

```go
testCases := []struct {
	name string

	// ...
}{
	// Validation fails the earliest, so none of the following dependencies are called.
	{
		name: "Error/Validation",

		input: invalidValue,

		expectErr: ErrValidation,
	},
	{
		name: "Error/Dependency1",

		input: validValue,

		dependencyMock1: &dependencyMock1{
			err: errFoo,
		},

		expectErr: errFoo,
	},
	{
		name: "Error/Dependency2",

		input: validValue,

		dependencyMock1: &dependencyMock1{},
		dependencyMock2: &dependencyMock2{
			err: errFoo,
		},

		expectErr: errFoo,
	}
}
```

Here, we gradually move the failure point from the beginning of the workflow to the end. Thus, each test case is the
continuation of the previous one.

Above, we only test for generic (unhandled) errors - here the `errFoo` comes in handy. However, you should also test
for handled errors. Test for handled errors before testing for generic errors.

```go
testCases := []struct {
	name string

	// ...
}{
	// Test specific error first.
	{
		name: "Error/Dependency1/NotFound",

		input: validValue,

		dependencyMock1: &dependencyMock1{
			err: ErrNotFound,
		},

		expectErr: ErrNotFound,
	},
	{
		name: "Error/Dependency1",

		input: validValue,

		dependencyMock1: &dependencyMock1{
			err: errFoo,
		},

		expectErr: errFoo,
	}
}
```

##### 3. Edge cases

Finally, whenever a bug is fixed, or for some very specific scenarios, you may add non-standard cases at the end.

#### Handling unpredictable values

Up until now, test cases were assumed to only produce / consume predictable values. Thhis is not always the case:
for example, you may need to deal with timestamps or auto-generated IDs.

> To keep the code simple, you should try your best to make unpredictable logic happen in a single layer, namely
> the `/service` (`/lib` also works to a certain extent). Upper layers should not bother with generating those,
> while lower layers should receive them statically â€“ in particular, don't generate values in postgres or
> other external connections. Pass them as parameters instead.

When this happens, you may use the `mock.MatchedBy` helper of mockery.

```go
// Replace
dependencyMock.EXPECT().
	Exec(
		mock.Anything,
		staticValue,
	)

// With
dependencyMock.EXPECT().
	Exec(
		mock.Anything,
		mock.MatchedBy(func (data *MyInput) bool {
			return assert.Equal(t, data.ID, staticValue.ID) &&
				// Unpredictable timestamp.
				assert.WithinDuration(t, time.Now(), data.Now, time.Second)
		})
	)
```

### Layers

Some test logic varies depending on the layer being tested.

#### DAO

Repository tests do not support parallel testing well (due to lock issues with multiple opened transactions on a
local instance). They are therefore exempt from the `t.Parallel()` directive.

To ensure isolation, each repository test should run in a special transaction, that will be automatically rolled back
at the end of the test.

```go
for _, testCase := range testCases {
	t.Run(testCase.name, func(t *testing.T) {
		postgres.RunTransactionalTest(t, config.PostgresPresetTest, func(ctx context.Context, t *testing.T) {
			t.Helper()

			db, err := postgres.GetContext(ctx)
			require.NoError(t, err)

			if len(testCase.fixtures) > 0 {
				_, err = db.NewInsert().Model(&testCase.fixtures).Exec(ctx)
				require.NoError(t, err)
			}

			// Rest of the test.
		})
	})
}
```

> DAO test cases usually define a `fixtures` field, which allows inserting records in the database before the repository
> is called. This array can contain any model that implements [bun models](https://bun.uptrace.dev/guide/models.html).
